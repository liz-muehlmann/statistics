<!DOCTYPE html><html><head>
      <title>ISL Chapter 2</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"],"imageFont":null},"root":"file:///C:/Users/lizmo/.atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax"});
        </script>
        <script type="text/javascript" async="" src="file:///C:\Users\lizmo\.atom\packages\markdown-preview-enhanced\node_modules\@shd101wyy\mume\dependencies\mathjax\MathJax.js" charset="UTF-8"></script>
        
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 id="chapter-2">Chapter 2</h1>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TYPE</td>
<td>notes</td>
</tr>
<tr class="even">
<td>BOOK</td>
<td>An Introduction to Statistical Learning</td>
</tr>
<tr class="odd">
<td>AUTHORS</td>
<td>Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani</td>
</tr>
<tr class="even">
<td>PUBLISHER</td>
<td>Springer</td>
</tr>
</tbody>
</table>
<p><strong>///////////////////////////////////////////////////////// Note: /////////////////////////////////////////////////////////</strong> <br> The .Rmd file was written and compiled using the Atom.io text editor. If you download the .Rmd file and try to open it in R Studio the LaTeX equations <em>will not</em> display properly. This has a easy fix: delete the spaces between the dollar signs ($). <br><br> Unfortunately, the R code chunks will not run properly in R Studio when downloading this .Rmd file. That is because the parameters listed inside the curly braces, {}, are incorrect. This fix is a little more time intensive, but is possible. For R studio the parameters take the form:</p>
<ul>
<li>{r loaddata, attr.source=‘.numberLines’}</li>
</ul>
<p>For Atom (using the Hydrogen and markdown-preview-enhanced packages), the paramaters take the form:</p>
<ul>
<li>r {cmd=“Rscript”, id=“loaddata”, .line-numbers}</li>
</ul>
<p>a useful guide for using R in Atom can be found here: <a href="https://jstaf.github.io/2018/03/25/atom-ide.html">R in Atom</a></p>
<ul>
<li>how to use Atom with Rmarkdown: <a href="https://shd101wyy.github.io/markdown-preview-enhanced/#/">Rmarkdown in Atom</a></li>
</ul>
<p>why? Atom has native <a href="github.io">Github</a> integration, the interface is cleaner, and you’re represented by an adorable <a href="https://myoctocat.com/">octocat</a>.You don’t need to use Atom. In this repo I’ve also included the PDF version of these notes. :) <br> <strong>///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////</strong></p>
<div class="code-chunk" data-id="loaddata" data-cmd="Rscript"><div class="input-div"><pre class="text language-text language-r line-numbers" data-role="codeBlock" data-info="r {code_chunk_offset=0, cmd=&quot;Rscript&quot;, id=&quot;loaddata&quot;, .line-numbers}"><span class="token comment"># the data from the book can be downloaded using install.packages("ISLR"). It's then loaded using the line below.</span>
library<span class="token punctuation">(</span>MASS<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></pre><div class="btn-group"><div class="run-btn btn"><span>▶︎</span></div><div class="run-all-btn btn">all</div></div><div class="status">running...</div></div><div class="output-div"></div></div>
<h2 id="what-is-statistical-learning">2.1 What is Statistical Learning?</h2>
<ul>
<li>$ X $ denotes the input variable (aka: predictor or independent variable)</li>
<li>$ Y $ denotes the output variable (aka: response or dependent variable)</li>
<li>The relationship between X and Y can be written as: $ Y=f(X)+\epsilon $</li>
<li>$ f $ is a fixed, but unknown function of $ X $ and $ \epsilon $ is the error term.</li>
<li>$ \epsilon $ is independent of $ X $ and has a mean of 0. The function $ f $ may take more than one input variable. (e.g.&nbsp;income ( $ y $ ) as function of education ( $ X_1 $ ) and seniority ($ X_2 $)).</li>
</ul>
<p>Statistics is all about ways to estimate $ f $</p>
<h3 id="why-estimate-f">2.1.1 Why Estimate <span class="math inline">f</span>?</h3>
<ol type="1">
<li>Prediction: this is when we know the values of $ X $, but can’t easily determine $ Y $.</li>
</ol>
<ul>
<li>$ \hat{y} = \hat{f}(x) $</li>
<li>$ \hat{Y} $ is the resulting predicting for $ Y $ (aka the predicted response)</li>
<li>$ \hat{f} $ is the estimate for $ f $.
<ul>
<li>It is a <em>black box</em> - where we don’t really care about $ \hat{f} $ as long as it gives accurate predictions for $ Y $.</li>
</ul></li>
</ul>
<p>Generally, $ \hat{f} $ will not be a perfect estimate $ f $, as a result the inaccuracy will introduce some error.</p>
<p><strong>Types of Error:</strong></p>
<ul>
<li><em>reducible error:</em> can be reduced to improve the accuracy of $ \hat{f} $ by using better statistical methods.</li>
<li><em>irreducible error:</em> since <span class="math inline">Y</span> is a function of $ \epsilon $, not all of the error can be reduced. Therefore there will always be $ \epsilon $</li>
</ul>
<p><strong>Reasons that $ \epsilon $ is not zero</strong></p>
<ul>
<li>$ \epsilon $ is not zero because it might include variables that are useful in predicting $ Y $.
<ul>
<li>Since we don’t measure these unincluded variables they can’t be predicted using $ f $.</li>
</ul></li>
<li>$ \epsilon $ may also contain unmeasurable variation, which also can’t be predicted using $ f $</li>
</ul>
<p>So if we have estimate $ \hat{f} $ and predictors $ X $ we get the prediction: $ \hat{Y} $ = $ \hat{f}(X) $.</p>
<p><strong>If we assume that $ \hat{f} $ and $ X $ are fixed:</strong></p>
<p>$ \begin{equation} \begin{split} E(Y-\hat{Y})^2 &amp;= E[f(X) - \epsilon - \hat{f}(X)]^2\cr &amp;=[f(X)-\hat{f}(X)]^2 + var(\epsilon) \end{split} \end{equation} $</p>
<ul>
<li>$ E=(Y-Y^2) $ is the expected value of the squared difference between the predicted value and actual value of $ Y $.</li>
<li>$ Var(\epsilon) $ is the variance associated with the error term $ \epsilon $</li>
</ul>
<p>The irreducible error gives an upper bound on the accuracy of our prediction for $ Y $ &amp; will almost always be unknown in practice.</p>
<ol start="2" type="1">
<li>Inference</li>
</ol>
<ul>
<li><em>Inference</em> is when we want to know how <span class="math inline">Y</span> is affected by change in the predictors, $ X_1…X_p $, but aren’t necessarily interested in making predictions for $ Y $.</li>
<li>the goal is to understand the relationship between $ X $ and <span class="math inline">Y</span>. How $ Y $ changes as a function of $ X_1…X_p $</li>
<li>$ \hat{f} $ can’t be treated as a <em>black box</em> because we have to know its exact form.</li>
<li>linear models are useful for inference.</li>
</ul>
<p><em>Inference is useful for:</em></p>
<ul>
<li>determining which predictors are associated with the outcome.</li>
<li>determining the relationship between the outcome and each predictor.</li>
<li>determining whether the relationship between $ Y $ and each predictor can be summarized using a linear equation or whether the relationship between the two is more complicated.</li>
</ul>
<h3 id="how-do-we-estimate-f">2.1.2 How do we Estimate <span class="math inline">f</span>?</h3>
<ul>
<li>$ n $ is the number of data points or observations we have.</li>
<li><em>training data</em> is a subset of the data we have that we use to train (or teach) the method how to estimate <span class="math inline">f</span>.</li>
<li>We apply a statistical learning method to the training data in order to estimate $ f $.</li>
<li>$ x_{ij} $ is the value of the <span class="math inline">j</span>th predictor for observation $ i $. $ i $ = 1, 2,…, $ n $ and $ j $ = 1, 2,…,$ p $ $ y_i $ is the response variable for $ i $th observation.</li>
<li>The training data would be $ {(x_i,y_i), (x_2, y_2),…(x_n,y_n)} $, where $ x = (x_{i1}, x_{i2},…, x_{ip})^T $.</li>
</ul>
<p>The goal is to find $ \hat{f} $ such that $ Y \approx \hat{f}(X) $ for any observation $ (X,Y) $</p>
<p><strong>There are two statistical learning methods we can use:</strong></p>
<ol type="1">
<li><em>Parametric:</em> to estimate $ f $ we only need to estimate one set of parameters.
<ul>
<li>the problem is that it will usually not match the true unknown form of $ f $</li>
<li>if the model is too far off from the true $ f $ (or the $ f $ using all the observations), the estimate will be poor</li>
<li>to solve poor fit, we can use more flexible models. But more flexible models requires estimating more parameters.</li>
<li>more complex models can lead to <em>overfitting:</em> which means they follow the errors too closely.</li>
<li>these involve a two-step model-based approach.</li>
</ul></li>
</ol>
<p><em>Step 1</em> We make an assumption of $ f $’s form. For example, if $ f $ is linear:</p>
<ul>
<li>$ f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + … + \beta_pX_p $</li>
<li>If $ f $ is linear, you only need to estimate the coefficients $ \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_pX_p $</li>
</ul>
<p><em>Step 2</em> We use the training data to <em>fit</em> or <em>train</em> the model. For the linear model we want to estimate: $ Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + … + \beta_pX_p $</p>
<p>One method of fitting the model is <em>(ordinary) least squares</em></p>
<ol start="2" type="1">
<li><p><em>Non-parametric:</em></p>
<ul>
<li>do not make explicit assumptions about the functional form of $ f $.</li>
<li>these methods try to get as close to the data points without being too rough.</li>
<li>since they don’t assume particular form of $ f $, they can fit a wider range of shapes for $ f $.</li>
<li>will fit the data better since it does not assume the form of $ f $.</li>
<li>requires substantially more observations in order to get an accurate estimate for <span class="math inline">f</span> than parametric approaches.</li>
</ul></li>
</ol>
<h3 id="the-trade-off-between-prediction-accuracy-and-model-interpretability">2.1.3 The Trade-off Between Prediction Accuracy and Model Interpretability</h3>
<p>Some methods are less flexible because they can produce only a small range of shapes to estimate <span class="math inline">f</span> (<em>e.g.</em> Linear regression can only create linear functions.)</p>
<ul>
<li>less flexible models are better for inference because they are more interpretable.</li>
<li>it’s easier to understand the relationship between $ Y $ and $ X_1, X_2,…X_p $ More flexible models include <em>thin plate splines</em> can generate a wider range of possible shapes to estimate <span class="math inline">f</span>.</li>
</ul>
<h3 id="supervised-vs-unsupervised-learning">2.1.4 Supervised vs.&nbsp;Unsupervised Learning</h3>
<p><em>Supervised Learning</em> for each observation of the predictor measurements $ x_i, i=1,…,n $ there is an associated response to the measurement $ y_i $.</p>
<ul>
<li>Includes: logistic regression, GAM, Boosting, and support vector machines</li>
</ul>
<p><em>Unsupervised Learning</em> is more complicated because for every observation $ i=1,…,n $ there’s a vector of measurements $ x_i $, but no associated response $ y_i $.</p>
<ul>
<li>it is called unsupervised because we have no response variable $ y $ to supervise our analysis.
<ul>
<li>Includes: cluster analysis (do the observations fall into relatively distinct groups?)</li>
</ul></li>
</ul>
<p><em>Semi-supervised Learning</em> is when you have observations for the predictors, but the corresponding measurements for the responses are less available.</p>
<h3 id="regression-vs-classification-problems">2.1.5 Regression vs.&nbsp;Classification Problems</h3>
<p><em>Quantitative Variables</em> are number variables. + problems involving Quantitative data are called <em>regression problems.</em></p>
<p><em>Qualitative Variables (aka categorical)</em> take on one of a number of <em>classes</em> or categories. + problems involving Qualitative data are called <em>classification</em> problems.</p>
<p>The difference between the two is not that clear cut and some methods (<em>K-nearest-neighbors</em> and <em>boosting</em>) can be used for both quantitative adn qualitative data.</p>
<p>Most statistical methods are based on whether the <em>response</em> is qualitative or quantitative.</p>
<h2 id="assessing-model-accuracy">2.2 Assessing Model Accuracy</h2>
<p>There is no <em>best</em> method in statistical learning. The data you have determines what kind of method you can (and should!) use.</p>
<h3 id="measuring-the-quality-of-fit">2.2.1 Measuring the Quality of Fit</h3>
<p>Measuring the quality of fit is a quantification of the extent to which the predicted response value for a given observation is close to the true response value of the observation.</p>
<p>For regressions, the most common measure is the <em>mean squared error (MSE).</em></p>
<ul>
<li>$ MSE = \frac {1}{n} \displaystyle \sum_{i=1}^{2}(y_i - \hat{f}(x_1)^2) $
<ul>
<li>$ \hat{f}(x_i) $ is the prediction $ \hat{f} $ gives for the $ i $th observation.</li>
</ul></li>
<li>a small test $ MSE $ is the goal
<ul>
<li>A small $ MSE $ means that the predicted responses are close to the true responses.</li>
</ul></li>
<li>We’re interested in how well our method works with previously unseen data (<em>i.e.,</em> not the training data.)</li>
<li>If we have training observations, which we use to fit obtain an estimate of $ \hat{f} $
<ul>
<li>$ {(x_1, y_1), (x_2, y_2), …, (x_n, y_n)} $</li>
</ul></li>
<li>We then compute $ \hat{f}(x_1), \hat{f}(x_2), …, \hat{f}(x_n) $
<ul>
<li>If these are approximately equal to $ y_1, y_2, …, y_n $ then the $ MSE $ is small.</li>
</ul></li>
<li>What we want to know is if $ \hat{f}(x_0) \approx y_0 $ where $ x_0 $ and $ y_0 $ is a previously unseen test observation <em>not used to train the statistical learning method.</em> Essentially, a new data point.
<ul>
<li>This is the <em>test</em> $ MSE $ which is what we’re interested in and the lower the better.</li>
</ul></li>
<li>For a large number of observations we can compute:
<ul>
<li>$ Ave(y_0 - \hat{f}(x_0))^2 $</li>
<li>The average squared prediction error for the test observations: $ (x_0, y_0) $</li>
<li>The smaller the number the better</li>
</ul></li>
<li>DO NOT rely on a method that results in a small <em>training</em> $ MSE $ even though the <em>test</em> and <em>training</em> $ MSE $ are closely related because most methods try to make a small training $ MSE $ which does not always result in a small <em>test</em> $ MSE $.
<ul>
<li><em>Overfitting</em> Is a method that gives as small <em>training</em> $ MSE $ but a large <em>test</em> $ MSE $.
<ul>
<li>It means that the method may be picking up patterns that are just random chance</li>
</ul></li>
</ul></li>
<li>The <em>training</em> $ MSE $ will generally be smaller than the <em>test</em> $ MSE $.</li>
<li><em>Cross-Validation</em> is a method for estimating the <em>test</em> $ MSE $ using the training data.</li>
</ul>
<h3 id="the-bias-variance-trade-off">2.2.2 The Bias Variance Trade-Off</h3>
<p>The expected <em>test</em> $ MSE $ (for a given value, $ x_0 $) is the sum of three fundamental quantities:</p>
<ol type="1">
<li>The <em>variance</em> of $ \hat{f}(x_0) $</li>
<li>The squared <em>bias</em> of $ \hat{f}(x_0) $</li>
<li>The variance of the error terms $ \epsilon $.</li>
</ol>
<ul>
<li>this is:
<ul>
<li>$ E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_))]^2 + Var(\epsilon) $
<ul>
<li>Where $ E(y_0 -\hat{f}(x_0)) $ is the <em>expected</em> test $ MSE $.
<ul>
<li>It is the average test $ MSE $ “we’d obtain if we repeatedly estimated $ f $ using a large number of training sets and tested each at $ x_0 $”</li>
</ul></li>
</ul></li>
</ul></li>
<li>The test $ MSE $ will never be below $ Var(\epsilon) $ (the irreducible error)</li>
<li>Our goal is to have <em>low variance</em> and <em>low bias</em> through the <em>bias-variance tradeoff</em>
<ul>
<li><em>Variance:</em> the amount that $ \hat{f} $ would change if we estimated it using a different training data set.
<ul>
<li>Ideally, we would not like $ \hat{f} $ to vary much between data sets.</li>
<li>a high variance would mean that small changes in the training data results in large changes in $ \hat{f} $.</li>
<li>More flexible methods will have higher variance.</li>
</ul></li>
<li><em>Bias:</em> is the error that is introduced by approximating real life using a simple model.
<ul>
<li>The closer real life is to the model used, the less bias there will be.</li>
<li>More flexible methods tend to have less bias.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="the-classification-setting">2.2.3 The Classification Setting</h3>
<p>This refers to problems where the outcome of interest is qualitative ( $ y_1, … y_n $)</p>
<p>The most common way to quantify the accuracy of the estimate $ \hat{f} $ is training the <em>error rate</em> - which is the proportion of mistakes that are made if we apply the estimated $ \hat{f} $ to the training observations.</p>
<p><em>Training error rate</em> equation is computed using the data that was used to train the classifier.</p>
<ul>
<li>$ \frac {1}{n} \displaystyle \sum_{i=1}^{n}I(y_i \neq \hat{y}_i) $
<ul>
<li>$ \hat{y}_1 $ is the predicted class label for the $ i $th observation using $ \hat{f} $</li>
<li>$ I(y_i \neq \hat{y}_i) $ is an <em>indicator variable</em>
<ul>
<li>$ y_i \neq \hat{y}_i $ is coded with a 1</li>
<li>$ y_i = \hat{y}_i $ is coded with a 0</li>
<li>if $ I(y_i \neq \hat{y}_i) = 0 $ then the $ i $th observation was classified correctly by the method, otherwise it was misclassified.</li>
</ul></li>
</ul></li>
</ul>
<p>We are still interested in the <em>test</em> error rate which is the error rate returned when we apply the classifier to test observations <em>not</em> used in training the data.</p>
<ul>
<li>The <em>test error</em> rate is:
<ul>
<li>$ Ave(I(y_0 \neq \hat{y}_0)) $
<ul>
<li>$ \hat{y}_0 $ is the predicted class label that results from applying the classifier to the test observation with predictor $ x_0 $</li>
</ul></li>
</ul></li>
</ul>
<p>We want the test error to be the smallest - which indicates we have a good classifier.</p>
<h4 id="the-bayes-classifier">The Bayes Classifier</h4>
<p>The test error rate is minimized (on average) by a simple classifier that assigns each observation to the most likely class, given its predictor values.</p>
<ul>
<li>we should assign a test observation with predictor vector $ x_0 $ to the class $ j $ for which the below is the largest
<ul>
<li>$ Pr(Y = j | X = x_0) $
<ul>
<li>This is a <em>conditional probability</em> equation.</li>
<li>It is the probability that $ Y = j $, given the observed predictor vector $ x_0 $</li>
</ul></li>
</ul></li>
</ul>
<p>This is <em>the Bayes classifier.</em> In a two-class problem it says that there are only two possible response values.</p>
<ul>
<li>it corresponds to predicting the first class if $ Pr(Y = 1 | X = x_0) &gt; 0.5 $, and class two otherwise.</li>
<li>The <em>Bayes decision boundary</em> is where the points have a probability of exactly 50%.
<ul>
<li>The <em>Bayes classifier</em> is determined by this boundary.</li>
</ul></li>
<li>The <em>Bayes classifier</em> produces the <em>Bayes error rate</em> which is the lowest possible test error rate.
<ul>
<li>The overall <em>Bayes Error Rate:</em> $ 1 - E( m\underset{j}axPr(Y = j | X)) $
<ul>
<li>where the expectation averages the probability over all possible values of $ X $</li>
<li>It is analogous to the irreducible error.</li>
</ul></li>
</ul></li>
</ul>
<p>Since we will never know the conditional distribution of $ Y $ given $ X $ in a real data set, the <em>Bayes classifier</em> is an unattainable gold standard against which we compare other models.</p>
<h4 id="k-nearest-neighbors">K-Nearest Neighbors</h4>
<p>Attempts to estimate the conditional distribution of $ Y $ given $ X $, then classifies a given observation to the class with the highest <em>estimated</em> probability.</p>
<ul>
<li>Given a positive integer $ K $ and a test observation $ x_0 $, $ KNN $:
<ol type="1">
<li>Identifies the $ K $ points in the training data that are closest to $ x_0 $ (represented by $ \mathcal{N}_0 $)</li>
<li>Estimates the conditional probability for class $ j $ as a fraction of points in $ \mathcal{N}_0 $ whose response value equals $ j $.</li>
</ol>
<ul>
<li>$ Pr(y = j | x = x_0) = \frac{1}{K} \underset{i\in\mathcal{N}_0}\sum I(y_i = j) $</li>
</ul>
<ol start="3" type="1">
<li>Applies Bayes rule and classifies the test observation $ x_0 $ to the class with the largest possibility.</li>
</ol></li>
<li>The user picks the value for $ K $.
<ul>
<li>A low $ K $ will produce a flexible model, but may have low bias but very high variance.</li>
<li>The higher the $ K $, the less flexible the model, which will result in low variance but high bias.</li>
</ul></li>
<li>The <em>training</em> error and the <em>test</em> error do not have a strong relationship.</li>
</ul>

      </div>
      <div class="md-sidebar-toc"><ul>
<li><a href="#chapter-2">Chapter 2</a>
<ul>
<li><a href="#what-is-statistical-learning">2.1 What is Statistical Learning?</a>
<ul>
<li><a href="#why-estimate-f">2.1.1 Why Estimate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>?</a></li>
<li><a href="#how-do-we-estimate-f">2.1.2 How do we Estimate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>?</a></li>
<li><a href="#the-trade-off-between-prediction-accuracy-and-model-interpretability">2.1.3 The Trade-off Between Prediction Accuracy and Model Interpretability</a></li>
<li><a href="#supervised-vs-unsupervised-learning">2.1.4 Supervised vs. Unsupervised Learning</a></li>
<li><a href="#regression-vs-classification-problems">2.1.5 Regression vs. Classification Problems</a></li>
</ul>
</li>
<li><a href="#assessing-model-accuracy">2.2 Assessing Model Accuracy</a>
<ul>
<li><a href="#measuring-the-quality-of-fit">2.2.1 Measuring the Quality of Fit</a></li>
<li><a href="#the-bias-variance-trade-off">2.2.2 The Bias Variance Trade-Off</a></li>
<li><a href="#the-classification-setting">2.2.3 The Classification Setting</a>
<ul>
<li><a href="#the-bayes-classifier">The Bayes Classifier</a></li>
<li><a href="#k-nearest-neighbors">K-Nearest Neighbors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
      <a id="sidebar-toc-btn">≡</a>
    
    
    
    
    
    
    
    
<script>

var sidebarTOCBtn = document.getElementById('sidebar-toc-btn')
sidebarTOCBtn.addEventListener('click', function(event) {
  event.stopPropagation()
  if (document.body.hasAttribute('html-show-sidebar-toc')) {
    document.body.removeAttribute('html-show-sidebar-toc')
  } else {
    document.body.setAttribute('html-show-sidebar-toc', true)
  }
})
</script>
      
  
    </body></html>